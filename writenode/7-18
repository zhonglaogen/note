hadoop2安装手顺（完全）
hadoop3.x:datadir(分文件夹，mount不同硬盘并发)
tmp（mapreduce，计算的临时结果）：mount（ssd）
radi0（磁盘上的raid卡）
1.备份
2.负载均衡
3.备份+负载均衡
bookeeper奇数（循环备份（圆圈））：每台数据都不全，1/3对外，其他备份其他两台
jounnode
secondary：tmp/current(fsimage,edit)
格式化namenode带上clusterID要和datanode一致，datanode通过这个寻找namenode
namespace和blockpoolID要一致，fedration，
datanode：finalized文件夹存data纯数据
.meta,文件块位置
备份虚拟机：ctrl+alt+t
总分，状态

hbase:
classpath：conf
内置zk，无法看错要设置为false

safe mode off才行
hbase shell
list看库
create ‘bb’ 
scan ‘bbb’

lsm-tree-hbase
1leveldb
2rocksdb
写过程读（读缓存）
hfile合并(平衡树)
region太大会分成两个（裂解）

命名空间list——namespace
create_namespace

ddl
dml
namespace
有ttl
hbase压力大，分布式habse，，
没有删，改，都是增加(版本区别)
合并hfile文件删除垃圾文件
修改
disable 锁上库，禁用
enable 解除
put 添加
scan 查看表所有
get查看某一个值
～～表结构的version

zookeeper只存位置信息
habse:数据写在hlog，Region是 HBase集群分布数据的最小单位，Region切分也会切分成两个列族文件。
hamster发送切分通知，regionserver切分，合并hfile（hfile会有小文件）
1.client访问hbase接口
0.96的三层架构
第一层是zookeeper中包含root region位置信息的节点，第二层是从root表中查找对应meta表的region的位置，第三层是从meta表中查找用户表对应region的位置。
0.96的两层架构
（1）用户通过查找zk（zookeeper）的/hbase/meta-region-server节点查询哪台RegionServer上有hbase:meta表。

（2）客户端连接含有hbase:meta表的RegionServer。Hbase:meta表存储了所有Region的行健范围信息，通过这个表就可以查询出你要存取的rowkey属于哪个Region的范围里面，以及这个Region又是属于哪个RegionServer。

（3）获取这些信息后，客户端就可以直连其中一台拥有你要存取的rowkey的RegionServer，并直接对其操作。

（4）客户端会把meta信息缓存起来，下次操作就不需要进行以上加载HBase:meta的步骤了。


 2.ZooKeeper 中记录 -ROOT- 表的 Location，.META. 和 -ROOT-(0.96以前)的位置 ，数据够用，减少请求次数，client访问zk得到region位置信息
3.store是属于列族，是不同的文件
4.memorystore（缓存）数据写到这里，在写道dfs中
数据先写到hlog中
每个列族store都有一个memestore缓存，数据先进缓存，满了形成hfile，hfile会合并
5以hfile的格式存储StoreFile
一个store一个列族，一个列族可对应多个store（region可切分）
高可用：
conf下创建 backup-master
添加内容 主机名
！！看到预分区

rowkey是按照字典顺序排序的
cell：rowkey，column family：column，version以字节码形式存储
读流程：client访问zk拿到meta表位置，通过meta表找到rowkey位置
写流程:client访问zk拿到meta表位置，通过meta找到要写的server，先写hlog，在写memstore，反馈，慢慢写
写比读块，读要访问storefile，写不用
写操作只有切分region时才更新meta，不切分默认无限大
memory store flush：
1.regionserver超过40%
2.memstore超过1h,所有的都flush
3.region所有内存加在一起超过128mb

compact（hfile合并）：
7天
一个store的超过三个hfile

真正的删除是合并时加载到本地删除
